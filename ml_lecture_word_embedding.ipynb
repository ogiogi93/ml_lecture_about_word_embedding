{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習入門講座 \n",
    "今回の講座では主に下記の項目にフォーカスしています\n",
    "- 機械学習を用いた文章間 類似度計算手法について、基礎的な仕組みを実際 データを分析することで体系的に理解する\n",
    "- ハンズオン形式で実施することで、機械学習・数学初心者でも手法 大まかな概要を理解することができる\n",
    "- Webサービスと機械学習手法の関わり方や機械学習 基本的な考え方を知ることで、受講者 スキルアップ・サービスへ 応用へ きっかけを提供する\n",
    "\n",
    "このチュートリアルでは、実際のデータを用いてコーディング・類似度計算を行うことで実践力をつけることを目的とします。今回はグノシーに実際に掲載されている記事データを使った文章間の類似度計算及びトピックモデルに関する実験を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文章間の類似度計算\n",
    "### 目的\n",
    "pythonの機械学習ライブラリの一つである「gensim」を使って、以下について体系的に学びます\n",
    "- Pythonによるデータ整形・分析処理について\n",
    "- word2vecを用いた文章間の類似度計算について\n",
    "- 類似度計算結果の可視化及びサービスへの応用について\n",
    "\n",
    "### 使用するデータ\n",
    "Web版グノシーに掲載されている記事データをクローリングし取得<br>\n",
    "クローリング方法については弊社のデータ分析ブログに細かく載っていますのでぜひご覧下さい<br>\n",
    "([Scrapy + Scrapy Cloudで快適Pythonクロール+スクレイピングライフを送る](http://data.gunosy.io/entry/python-scrapy-scraping))\n",
    "\n",
    "### 講師\n",
    "株式会社Gunosy 開発本部 データ分析部 荻原 崇 (FB: [ogiogi93](https://www.facebook.com/ogiogi93))<br>\n",
    "開発本部データ分析部所属。2017年度新卒入社。大学院ではセンサを用いた人々の流動分析およびJR東日本との共同プロジェクトにて、電車の乗車率予測モデルに関する研究を行う。現在は動画配信プロジェクトのロジック、データ分析全般を担当。調理師免許保持者。\n",
    "\n",
    "### アジェンダ\n",
    "1. バックグラウンド\n",
    "2. ライブラリのimportとデータの準備\n",
    "3. 類似度計算を実施するためのデータの整形\n",
    "4. word2vecによるモデルの生成と単語間での類似度計算\n",
    "5. doc2vecによるモデルの生成と文章間での類似度計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. バックグラウンド\n",
    "近年, 機械学習などによるデータ分析に注目が集まり、既に多くの業界・サービスに応用されています。数ある機械学習手法の中でも、word2vecやLDAは「検索・レコメンド・評判分析」など様々なタスクに応用することができます。そのため、これらの手法をまず身につけることで様々なタスク・領域で機械学習を応用することが可能となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.accel-brain.com/wp-content/uploads/2016/03/linear-relationships.png#link2keyword=%3Cimg%20class%3D%22aligncenter%20size-medium%20wp-image-951%22%20src%3D%22%2F%2Fmedia.accel-brain.com%2Fwp-content%2Fuploads%2F2016%2F03%2Flinear-relationships-500x175.png%22%20alt%3D%22linear-relationships%22%20width%3D%22500%22%20height%3D%22175%22%20%2F%3E\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://media.accel-brain.com/wp-content/uploads/2016/03/linear-relationships.png#link2keyword=%3Cimg%20class%3D%22aligncenter%20size-medium%20wp-image-951%22%20src%3D%22%2F%2Fmedia.accel-brain.com%2Fwp-content%2Fuploads%2F2016%2F03%2Flinear-relationships-500x175.png%22%20alt%3D%22linear-relationships%22%20width%3D%22500%22%20height%3D%22175%22%20%2F%3E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインポートとデータの準備 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 必要ライブラリをimport\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import igo\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word2vec が実装されている　gensim というライブラリを用いる\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import word2vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グノシーから取得した記事データをPandasのDataFrameに読み込みます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv('csv/item_gunosy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを確認すると、記事の\n",
    "- title: タイトル\n",
    "- subcategory: サブカテゴリー\n",
    "- thumbnail_url: サムネイルのURL\n",
    "- url: 記事のURL\n",
    "が含まれていることが確認できます。また、今回の記事データは一部サムネイルURLがない記事も存在します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>thumbnail_url</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GunosynewsItem</td>\n",
       "      <td>ゲーム</td>\n",
       "      <td>images.gunosy.com/6/11/abf91413c25e9329d0a334410d8d007e_large.jpg</td>\n",
       "      <td>ワイワイ系の懐かしトイが超絶進化！「東京おもちゃショー」で見つけた傑作選</td>\n",
       "      <td>https://gunosy.com/articles/Ru6Ac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _type subcategory  \\\n",
       "0  GunosynewsItem  ゲーム          \n",
       "\n",
       "                                                       thumbnail_url  \\\n",
       "0  images.gunosy.com/6/11/abf91413c25e9329d0a334410d8d007e_large.jpg   \n",
       "\n",
       "                                  title                                url  \n",
       "0  ワイワイ系の懐かしトイが超絶進化！「東京おもちゃショー」で見つけた傑作選  https://gunosy.com/articles/Ru6Ac  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. 類似度計算を実施するためのデータの整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensimのword2vecに記事データを入力するためには、データを整形する必要があります"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 日本語文字列の分かち書き"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語の文章をword2vecによってベクトル化するためには、まず文章を単語に分ける必要があります<br>\n",
    "今回はMecabではなく、[igo-python](https://pypi.python.org/pypi/igo-python/)を用いて分かち書きを行います。\n",
    "また、辞書は[mecab-ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd)を利用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger = igo.tagger.Tagger('dic/neologd/')\n",
    "\n",
    "def parser(text):\n",
    "    \"\"\" 　テキストを構文解析する関数 \"\"\"\n",
    "    return tagger.parse(text)\n",
    "\n",
    "def extract_target_morphs(sentence):\n",
    "    \"\"\"対象の形態素をリストで返却する関数\"\"\"\n",
    "    morphs = []\n",
    "    # タイトル文を構文解析する\n",
    "    for morph in parser(sentence):\n",
    "        # 構文解析結果が空であった場合スキップする\n",
    "        if not morph.surface.strip():\n",
    "            continue\n",
    "        lemma = morph.surface\n",
    "        data = morph.feature.split(',')\n",
    "        if data[0] in ['動詞', '名詞']or (data[0] == '形容詞' and data[6] != '*'):\n",
    "            morphs.append(data[6])\n",
    "    return morphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "助詞や記号などは文章内で多く出現し、それほど意味を持たないため除くことが多いです(これらのワードをストップワードと呼びます)<br>\n",
    "今回も動詞・名詞・形容詞に絞って分かち書きを行います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['困惑', 'てんや', '謎', 'メニュー', '豚', '角', '煮る', '天丼', '普通に', 'ウマ', 'いる']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_target_morphs('困惑】「てんや」の謎の新メニュー『豚角煮天丼』、普通にウマい')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記事タイトルを分かち書きする\n",
    "さらに、word2vec に入力するために構文解析(分かち書き)済みのタイトルをスペースで分割する "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 記事タイトルを分かち書きする\n",
    "df_articles['parsed_title'] = df_articles['title'].apply(extract_target_morphs)\n",
    "df_articles['parsed_title'] = df_articles['parsed_title'].apply(lambda x:  [' '.join(v for v in x)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>parsed_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ワイワイ系の懐かしトイが超絶進化！「東京おもちゃショー」で見つけた傑作選</td>\n",
       "      <td>ゲーム</td>\n",
       "      <td>ワイ ワイ 系 懐かしい トイ 超絶 進化 東京おもちゃショー 見つける 傑作 選</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title subcategory  \\\n",
       "0  ワイワイ系の懐かしトイが超絶進化！「東京おもちゃショー」で見つけた傑作選  ゲーム          \n",
       "\n",
       "                                parsed_title  \n",
       "0  ワイ ワイ 系 懐かしい トイ 超絶 進化 東京おもちゃショー 見つける 傑作 選  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles[['title', 'subcategory', 'parsed_title']].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書き済みのタイトル文字群を一旦txtファイルに保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/titles.txt', 'w') as f:\n",
    "    df_articles['parsed_title'].to_csv(f, header=None, index=None, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word2vecによるモデルの生成と類似度計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はword2vecのモデルの一つである、「Skip-gram」を用いて単語間の類似度計算を考えてみます。「Skip-gram」とは、「ある単語が入力された時に、その単語の周辺にどのような単語が出現しやすいか」を予測するタスクを解くモデルになります<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば「【ソース不要！】じゅわトロ「トマトのシンプルピザ」を作ってみない？」という文章について、「ピザ」という単語に注目した時、「トマト」や「チーズ」などのビザの食材が\n",
    "周辺に多く出現する可能性が高いです。「Skip-gram」はこの周辺に出やすい単語の出現確率について、指定された単語分算出することでベクトルに変換します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 構文解析済みタイトルの文字群をword2vecにかける\n",
    "titles = w2v.LineSentence('data/titles.txt')\n",
    "model = w2v.Word2Vec(titles, size=100, window=5, min_count=1, sg=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('豆腐', 0.9994983673095703),\n",
       " ('面白い', 0.9994969964027405),\n",
       " ('朝', 0.9994852542877197),\n",
       " ('運命', 0.9994773268699646),\n",
       " ('似合う', 0.999453604221344),\n",
       " ('忙しい', 0.9994499683380127),\n",
       " ('笑', 0.9994399547576904),\n",
       " ('真ん中', 0.9994328022003174),\n",
       " ('直伝', 0.9994291067123413),\n",
       " ('火', 0.9994194507598877)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ある単語に最も類似度が高い単語を返す\n",
    "model.most_similar(positive='ご飯')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はトレーニングデータが少ないことから、精度があまりよくはありません。。<br>\n",
    "そこで、株式会社白ヤギコーポーレーションから学習済みword2vecモデルが公開されているため、そのモデルを使用してみます<br>[word2vecの学習済み日本語モデルを公開します](http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 白ヤギコーポレーションから公開されている学習済みのモデルをロードする\n",
    "model_path = 'model/word2vec.gensim.model'\n",
    "model = w2v.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('タレ', 0.9213054180145264),\n",
       " ('汁', 0.917236328125),\n",
       " ('豆腐', 0.9129927754402161),\n",
       " ('鍋', 0.9093849658966064),\n",
       " ('味噌汁', 0.9085185527801514),\n",
       " ('油揚げ', 0.8977984189987183),\n",
       " ('おにぎり', 0.8972976803779602),\n",
       " ('梅干し', 0.893747866153717),\n",
       " ('汁物', 0.8927422761917114),\n",
       " ('天ぷら', 0.8925649523735046)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ある単語に最も類似度が高い単語を返す\n",
    "model.most_similar(positive='ご飯')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11716078, -0.07515609,  0.00853589, -0.08632582,  0.09497195,\n",
       "        0.13244595, -0.13451752, -0.08669069,  0.11902761,  0.04120288,\n",
       "        0.20926368,  0.1081875 , -0.06735694,  0.37801969, -0.11764119,\n",
       "        0.18195644,  0.09917535, -0.11339456, -0.07504001,  0.1889106 ,\n",
       "       -0.03481169, -0.079282  ,  0.09465183,  0.14899369, -0.12081202,\n",
       "       -0.23041828,  0.08662395,  0.32221848, -0.08037522,  0.14042863,\n",
       "        0.08189129,  0.04125   ,  0.12892513,  0.05284416,  0.04760501,\n",
       "        0.04229622, -0.0771471 ,  0.03095818,  0.08163084, -0.24840046,\n",
       "       -0.07823882, -0.09401023, -0.09574612,  0.06188807, -0.21567668,\n",
       "        0.13368349, -0.00879957,  0.25245234,  0.18297784, -0.22478935], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各単語に指定したサイズのベクトルが計算されている(size=50)\n",
    "model.wv['ラーメン']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各単語に上記のような50次元の単語ベクトルが割り当てられました。これらの単語ベクトルを平均化することで、文章間での類似度も計算することが可能となります！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語ベクトルの平均値から文章間での類似度を計算してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVector(object):\n",
    "    def __init__(self, df, title):\n",
    "        self.df = df\n",
    "        self.title = title\n",
    "        self.target_article_vector = None\n",
    "        \n",
    "    def path_to_image_html(self, path):\n",
    "        return '<img src=\"http://'+ path + '\"/>'\n",
    "    \n",
    "    def mean_embedding_vector(self, words):\n",
    "        \"\"\" 文章内の単語ベクトルの平均値を算出する \"\"\"\n",
    "        featureVec = np.zeros((model.vector_size,), dtype=\"float32\")\n",
    "        nwords = 0\n",
    "        # もしタイトル内の単語がモデルのボキャブラリーに含まれていた場合、その単語のベクトルをlistに追加する\n",
    "        for word in words:\n",
    "            if word in model.wv.vocab:\n",
    "                nwords += 1\n",
    "                featureVec = np.add(featureVec, model.wv[word])\n",
    "        # ボキャブラリーに含まれていた単語数で割ることで、単語ベクトルの平均を算出\n",
    "        if(nwords>0):\n",
    "             featureVec = np.divide(featureVec, nwords)\n",
    "        return list(set(featureVec))\n",
    "\n",
    "    def cosine_similarity(self, other_article_vector):\n",
    "        \"\"\" cos類似度を計算する \"\"\"\n",
    "        return 1 - spatial.distance.cosine(self.target_article_vector, other_article_vector)\n",
    "\n",
    "    def similer_titles_by_mean_embedding_vector(self, df_other_article):\n",
    "        \"\"\" 入力されたタイトルに近しい記事を出力する\"\"\"\n",
    "        # 比較対象元の単語ベクトル平均と比較対象の単語ベクトル平均のcos類似度を計算する\n",
    "        df_other_article['score'] = df_other_article['mean_vector'].apply(self.cosine_similarity)\n",
    "        \n",
    "        # cos類似度が高い順にソートする\n",
    "        return df_other_article.sort_values('score', ascending=False).reset_index().head(5)\n",
    "\n",
    "    def plot_similer_articles(self):\n",
    "        \"\"\" 入力されたタイトルに近しい記事を計算し、可視化する\"\"\"\n",
    "        self.df['thumbnail'] = self.df['thumbnail_url'].apply(self.path_to_image_html)\n",
    "        self.df['mean_vector'] = self.df['parsed_title'].apply(self.mean_embedding_vector)\n",
    "        \n",
    "        # 比較元の単語ベクトルの平均値を代入する\n",
    "        df_target_article = self.df[self.df['title'] == self.title].reset_index()\n",
    "        df_target_article['score'] = None\n",
    "        \n",
    "        # 入力されたタイトルが記事データ内にない場合、新たにベクトルを生成し類似度を計算する\n",
    "        if df_target_article.empty:\n",
    "            parsed_title = extract_target_morphs(self.title)\n",
    "            self.target_article_vector = self.mean_embedding_vector(parsed_title)\n",
    "            df_similer_articles = self.similer_titles_by_mean_embedding_vector(df_other_article = self.df)\n",
    "            print('比較対象元のタイトル: {}'.format(self.title))\n",
    "            return HTML(df_similer_articles[['title', 'thumbnail', 'score', 'parsed_title']].to_html(escape=False))\n",
    "        \n",
    "        self.target_article_vector = df_target_article['mean_vector'][0]\n",
    "        df_similer_articles = self.similer_titles_by_mean_embedding_vector(df_other_article = self.df[self.df['title'] != self.title].reset_index(drop=True))\n",
    "        df_similer_articles = pd.concat([df_target_article, df_similer_articles], axis=0)\n",
    "        return HTML(df_similer_articles[['title', 'thumbnail', 'score', 'parsed_title']].to_html(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "比較対象元のタイトル: ラーメン\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>score</th>\n",
       "      <th>parsed_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【食卓の人気者に！】「鶏肉のケチャップ醤油焼き」がやみつきの旨さ</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/3/e557fc993068cbcceec32f79c52808b6_large.jpg\"/></td>\n",
       "      <td>0.547929</td>\n",
       "      <td>食卓 人気 者 鶏肉 ケチャップ 醤油 焼き 病み付き 旨い さ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>松屋が「大創業祭」 「牛めし」50円引き</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/9/898568695846f9b39ac11b27cd671cad_large.jpg\"/></td>\n",
       "      <td>0.547368</td>\n",
       "      <td>松屋 創業 祭 牛めし 50円 引き</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本で発明されたあの食べ物が、とにもかくにも大好きな中国の人たち=中国メディア</td>\n",
       "      <td><img src=\"http://images.gunosy.com/5/30/663df87d678b54f52a63212005c8d89e_large.jpg\"/></td>\n",
       "      <td>0.526601</td>\n",
       "      <td>日本 発明 する れる 食べ物 大好き 中国 人達 中国メディア</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>セ・リーグ個人別本塁打成績  (9日現在)</td>\n",
       "      <td><img src=\"http://null\"/></td>\n",
       "      <td>0.522266</td>\n",
       "      <td>セ・リーグ 個人 別 本塁打 成績 9日 現在</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【丸亀製麺】6月6日から3日間『牛とろ玉うどん』が半額の340円に！ これ本当に美味しいやつですよ【夜なきうどん】</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/3/7d21cc49226037eb0ff36bc1632f1514_large.jpg\"/></td>\n",
       "      <td>0.519418</td>\n",
       "      <td>丸亀製麺 6月6日 3日 間 牛 吐露 玉 うどん 半額 * 円 これ 美味しい やつ ですよ。 夜 ない うどん</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanEmbeddingVector(df=df_articles, title='ラーメン').plot_similer_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにword2vecは単語間の類似度を算出することができ、さらに各単語のベクトルの平均値などを用いることで文章間の類似度を計算することが可能です。しかし、単語の並び順が考慮されないという欠点があります。<br>\n",
    "そこで、文章間の類似度を計算するためにword2vecを拡張したdoc2vecという手法が提案されています。doc2vecは単語の並び順も考慮した文章間の類似度計算を実施することが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.doc2vecによるモデルの生成と類似度計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vecもword2vecと同様、予め文章を分かち分けし単語に分けたデータを入力します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc2vec用のライブラリをimportする\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#１文書ずつ、単語に分割してリストに入れていく\n",
    "#words：文書に含まれる単語のリスト（単語の重複あり）\n",
    "# tags：文書の識別子（リストで指定．1つの文書に複数のタグを付与できる）\n",
    "sentences = [TaggedDocument(words = data.split(),tags = [i]) for i,data in enumerate(open('data/titles.txt','r'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['ワイ', 'ワイ', '系', '懐かしい', 'トイ', '超絶', '進化', '東京おもちゃショー', '見つける', '傑作', '選'], tags=[0]),\n",
       " TaggedDocument(words=['タモリ', '黄金', 'シャチホコ', '前'], tags=[1])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[([単語1,単語2,単語3],文書id),...]というような行が並んでいます\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのパラメータは以下についてです\n",
    "- size: ベクトル化した際の次元数\n",
    "- alpha: 学習率　(低いほど収束が早いですが、精度が落ちてしまいます)\n",
    "- sample: 単語を無視する際の頻度の閾値 (多くの文章に登場する単語はそれほど重要ではない可能性があるため、このパラーメータで閾値を設定します)\n",
    "- min_count: 学習に使う単語の最低出現回数 (sampleとは逆に出現頻度が非常に少ない単語についてもそれほど重要ではない可能性があるため、このパラメータで設定します)\n",
    "- workers: 学習時のスレッド数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_doc = models.Doc2Vec(dm=0, size=300, window=15, alpha=.025, min_alpha=.025, min_count=1, sample=1e-6, workers=4)\n",
    "model_doc.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に記事タイトルから文章間の類似度計算を実験してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def path_to_image_html(path):\n",
    "    return '<img src=\"http://'+ path + '\"/>'\n",
    "\n",
    "def similer_titles(df, title):\n",
    "    \"\"\" 入力されたタイトルに近しい記事を出力する\"\"\"\n",
    "    # 分かち書きされたタイトルの単語群から新しいベクトルを生成する\n",
    "    new_vector = model_doc.infer_vector(title)\n",
    "    # ベクトルが近いタイトルのidを取得する\n",
    "    sims = model_doc.docvecs.most_similar([new_vector])\n",
    "    # 類似度が高いタイトルに絞る\n",
    "    df_similer_articles = df.loc[[x[0] for x in sims], :]\n",
    "    # Scoreも入力しておく\n",
    "    df_similer_articles['score'] = [x[1] for x in sims]\n",
    "    return df_similer_articles.head(5)\n",
    "\n",
    "def plot_similer_articles(df, title):\n",
    "    \"\"\" 入力されたタイトルに近しい記事を計算し、可視化する\"\"\"\n",
    "    df['thumbnail'] = df['thumbnail_url'].apply(path_to_image_html)\n",
    "    target = df[df['title'] == title].reset_index()\n",
    "    if target.empty:\n",
    "        parsed_title = extract_target_morphs(title)\n",
    "        df_similer_articles = similer_titles(df=df, title=parsed_title)\n",
    "        print('比較元文章: {}'.format(title))\n",
    "        return HTML(df_similer_articles[['title', 'thumbnail', 'score', 'parsed_title']].to_html(escape=False))\n",
    "    \n",
    "    target['score'] = None\n",
    "    df_similer_articles = similer_titles(df=df[df['title'] != title], title=target['parsed_title'])\n",
    "    df_similer_articles = pd.concat([target, df_similer_articles], axis=0)\n",
    "    return HTML(df_similer_articles[['title', 'thumbnail', 'score', 'parsed_title']].to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は学習データが少ないため、word2vecと比べ精度は非常に悪いですね"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "比較元文章: ラーメン食べたいな\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>score</th>\n",
       "      <th>parsed_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>関門海峡を隔てて伝える戦と信仰の歴史~下関・門司~</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/11/ed3a4f6111e2d5540cd84f75648088cf_large.jpg\"/></td>\n",
       "      <td>0.226589</td>\n",
       "      <td>関門海峡 隔てる 伝える 戦 信仰 歴史 下関 門司</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>韓国大統領特使への安倍首相のもてなしがひどい！韓国ネットユーザーが椅子に注目し指摘=「ミスにしてはあからさま」「韓国を相当見下してるね」</td>\n",
       "      <td><img src=\"http://images.gunosy.com/5/24/9fb111aca485b93fb0583b2e6e32489c_large.jpg\"/></td>\n",
       "      <td>0.210238</td>\n",
       "      <td>韓国大統領 特使 安倍首相 もてなし ひどい 韓国 ネットユーザー 椅子 注目 する 指摘 ミス 仕手 あからさま 韓国 見下す てる</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>肩こりも頭痛も「うつ伏せで腰を揺らす」だけで改善するってホント？</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/10/9ce5de22706babe29d1efe4f46525042_large.jpg\"/></td>\n",
       "      <td>0.206390</td>\n",
       "      <td>肩こり 頭痛 うつ 伏せ 腰 揺らす 改善 する ホント</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114</th>\n",
       "      <td>済州戦の騒動で約220万円の罰金...浦和が声明「今後の対応は改めて報告」</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/10/034c46c449a998be7257a620058c62e8_large.jpg\"/></td>\n",
       "      <td>0.203589</td>\n",
       "      <td>済州 戦 騒動 * 万 円 罰金 浦和 声明 今後 対応 報告</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>初夏のムードにピッタリのジューシーな”シトラスカラーネイル”</td>\n",
       "      <td><img src=\"http://images.gunosy.com/6/11/22d4eaa71463cdbab56cbea493ada10f_large.jpg\"/></td>\n",
       "      <td>0.202512</td>\n",
       "      <td>初夏 ムード ジューシー *</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_similer_articles(df_articles, 'ラーメン食べたいな')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
